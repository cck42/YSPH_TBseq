{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af897a67-300d-48bb-ac34-5ad746435b05",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'samtools_depth_indiv_primers/S.agalactiae_fwd.depth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m rev_depth_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamtools_depth_indiv_primers\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgpsc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_rev.depth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# load each depth file into a df\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m fwd_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfwd_depth_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRef\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDepth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m fwd_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrimer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfwd\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     48\u001b[0m rev_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(rev_depth_file, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRef\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPos\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDepth\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'samtools_depth_indiv_primers/S.agalactiae_fwd.depth'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "# load in config with GPSC paths\n",
    "with open(\"config/TB_reps.yaml\", 'r') as file:\n",
    "    config=yaml.safe_load(file)\n",
    "\n",
    "# extract GPSCs\n",
    "gpscs = config['samples'] \n",
    "\n",
    "# set params\n",
    "amplicon_stats = list()\n",
    "xlen = 2200\n",
    "\n",
    "total_genome_coverages={}\n",
    "amplicon_positions={}\n",
    "\n",
    "with open('output.txt', 'w') as f:\n",
    "\n",
    "    for gpsc, fasta_file in gpscs.items():\n",
    "        records = list(SeqIO.parse(fasta_file, \"fasta\"))\n",
    "\n",
    "        # calculate length of the genome for each GPSC\n",
    "        genome_length=sum(len(record.seq) for record in records)\n",
    "\n",
    "        # initialize amplicon genome coverage for each GPSC separately \n",
    "        total_genome_coverage=0\n",
    "\n",
    "        # initialize set of covered positions across each GPSC \n",
    "        covered_positions = set()\n",
    "\n",
    "        # intitialize list of amplicon positions \n",
    "        amplicon_positions[gpsc] = []\n",
    "\n",
    "        print(f\"Processing {gpsc}...\", file=f)  \n",
    "\n",
    "        # load the samtools depth file\n",
    "        fwd_depth_file = os.path.join(\"samtools_depth_indiv_primers\", f\"{gpsc}_fwd.depth\")\n",
    "        rev_depth_file = os.path.join(\"samtools_depth_indiv_primers\", f\"{gpsc}_rev.depth\")\n",
    "        \n",
    "        # load each depth file into a df\n",
    "        fwd_df = pd.read_csv(fwd_depth_file, sep=\"\\t\", names=[\"Ref\", \"Pos\", \"Depth\"])\n",
    "        fwd_df['Primer'] = 'fwd'\n",
    "\n",
    "        rev_df = pd.read_csv(rev_depth_file, sep=\"\\t\", names=[\"Ref\", \"Pos\", \"Depth\"])\n",
    "        rev_df['Primer'] = 'rev'\n",
    "\n",
    "        # combine the dfs\n",
    "        df = pd.concat([fwd_df, rev_df])\n",
    "\n",
    "        # filter for fwd and rev positions separately \n",
    "        fwd_primer_binding_sites = df[(df[\"Depth\"] == 1) & (df[\"Primer\"] == 'fwd')][\"Pos\"].tolist()\n",
    "\n",
    "        rev_primer_binding_sites = df[(df[\"Depth\"] == 1) & (df[\"Primer\"] == 'rev')][\"Pos\"].tolist()\n",
    "\n",
    "        def calculate_amplicons(primer_binding_sites, gpsc, xlen, amplicon_positions, amplicon_stats, covered_positions, total_genome_coverage, f, primer_direction):\n",
    "            for p1loc in primer_binding_sites:\n",
    "        # find the next primer binding site within xlen bases\n",
    "                p2loc = next((pos for pos in primer_binding_sites if p1loc < pos <= p1loc + xlen), None)\n",
    "\n",
    "                if p2loc is not None:\n",
    "            # amp stats\n",
    "                    amplicon_stats.append((gpsc, gpsc, gpsc, gpsc, gpsc, gpsc, 0, 0, 0, p1loc, p2loc, primer_direction))\n",
    "                    covered_positions.update(range(p1loc, p2loc+1))\n",
    "            # ID amp positions\n",
    "                    amplicon_positions[gpsc].append((p1loc, p2loc))\n",
    "                    print(f\"Detected amplicon from {p1loc} to {p2loc}.\", file=f)  \n",
    "\n",
    "            # get total genome coverage\n",
    "                    total_genome_coverage += p2loc - p1loc\n",
    "\n",
    "        # calculate predicted % coverage \n",
    "            coverage_percentage = (len(covered_positions) / genome_length) * 100\n",
    "\n",
    "        # update dictionary \n",
    "            total_genome_coverages[gpsc] = coverage_percentage\n",
    "\n",
    "        # summarize fwd amplicons\n",
    "        calculate_amplicons(fwd_primer_binding_sites, gpsc, xlen, amplicon_positions, amplicon_stats, covered_positions, total_genome_coverage, f, 'fwd')\n",
    "        # summarize rev amplicons\n",
    "        calculate_amplicons(rev_primer_binding_sites, gpsc, xlen, amplicon_positions, amplicon_stats, covered_positions, total_genome_coverage, f, 'rev')\n",
    "\n",
    "colnames = [\"pid1\", \"pid2\", \"set1\", \"set2\",\n",
    "            \"pseq1\", \"pseq2\",\n",
    "            \"max_hdist\", \"hdist1\", \"hdist2\",\n",
    "            \"p1loc\", \"p2loc\", \"PrimerDirection\"]\n",
    "coltypes = [\"<U30\", \"<U30\", \"<U30\", \"<U30\",\n",
    "            \"<U30\", \"<U30\",\n",
    "            float, float, float,\n",
    "            int, int, \"<U3\"]\n",
    "\n",
    "dt = {'names': colnames, 'formats': coltypes}\n",
    "\n",
    "amplicon_statsnp = np.array(amplicon_stats,\n",
    "                     dtype=dt)\n",
    "\n",
    "np.save(\"amplicon_statstab.npy\", amplicon_statsnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12e8e49b-c9d3-486c-ac46-71f99dc8bd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Initialize a dictionary to group amplicons by sequence and primer type\n",
    "grouped_amplicons = {}\n",
    "\n",
    "# Define the maximum allowed size for amplicons\n",
    "max_amplicon_size = 2000\n",
    "\n",
    "# Iterate over amplicon_stats, assuming the last element is primer_direction, and the first is gpsc\n",
    "for amplicon in amplicon_stats:\n",
    "    *_, p1loc, p2loc, primer_direction = amplicon\n",
    "    gpsc = amplicon[0]\n",
    "    key = (gpsc, primer_direction)\n",
    "    if key not in grouped_amplicons:\n",
    "        grouped_amplicons[key] = []\n",
    "    grouped_amplicons[key].append((p1loc, p2loc))\n",
    "\n",
    "# Initialize list for contiguous amplicons\n",
    "contiguous_amplicons = []\n",
    "\n",
    "# Process each group for contiguity\n",
    "for (gpsc, primer_direction), positions in grouped_amplicons.items():\n",
    "    positions.sort()\n",
    "    start, end = positions[0]\n",
    "    \n",
    "    for current_start, current_end in positions[1:]:\n",
    "        # Check if contiguous and does not exceed max amplicon size\n",
    "        if current_start <= end + 1 and (current_end - start) <= max_amplicon_size:\n",
    "            end = max(end, current_end)\n",
    "        else:\n",
    "            contiguous_amplicons.append([gpsc, start, end, primer_direction])\n",
    "            start, end = current_start, current_end\n",
    "    # After processing all positions, append the last contiguous segment\n",
    "    contiguous_amplicons.append([gpsc, start, end, primer_direction])\n",
    "\n",
    "# Write contiguous amplicons to CSV\n",
    "with open('amplicon_positions.csv', 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow([\"Sequence\", \"Start\", \"End\", \"Primer Type\"])\n",
    "    for amplicon in contiguous_amplicons:\n",
    "        csvwriter.writerow(amplicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc50ee06-71bd-4239-bec0-4bffa3581bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "            ------------------------------------------------------ separate fwd and rev amplicon predictions --------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79f1ddc9-c924-4d74-bb5f-7745bf8f7297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create csv file containing predicted genome coverage for each sequence based on predicted amplicon coverage\n",
    "total_genome_coverages_df = pd.DataFrame([total_genome_coverages])\n",
    "\n",
    "total_genome_coverages_df.to_csv('genome_coverage_pc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d221bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H.influenzae: 0.003879488869145387%\n",
      "H37Rv: 94.29513375398841%\n",
      "K.pneumoniae: 0.03167923478560163%\n",
      "M.abscessus: 0.39467774135158623%\n",
      "M.canetti: 89.43826698968014%\n",
      "M.fortuitum: 0.5697250983129756%\n",
      "M.intracellulare: 2.1540788708430068%\n",
      "OXC141: 0.005449545797541027%\n",
      "PA01: 0.03133578230267396%\n"
     ]
    }
   ],
   "source": [
    "for gpsc, coverage in total_genome_coverages.items():\n",
    "    print(f\"{gpsc}: {coverage}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b6f337d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H.influenzae: 1830138 bp\n",
      "H37Rv: 4411532 bp\n",
      "K.pneumoniae: 5438894 bp\n",
      "M.abscessus: 5067172 bp\n",
      "M.canetti: 4432426 bp\n",
      "M.fortuitum: 6406072 bp\n",
      "M.intracellulare: 5402402 bp\n",
      "OXC141: 2036867 bp\n",
      "PA01: 6264404 bp\n",
      "S.aureus: 2872762 bp\n",
      "S.odontolytica: 2455831 bp\n"
     ]
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "# Load the config file\n",
    "with open(\"config/TB_reps.yaml\", 'r') as file:\n",
    "    config=yaml.safe_load(file)\n",
    "\n",
    "# Extract the GPSCs\n",
    "gpscs = config['samples']\n",
    "\n",
    "# Initialize a dictionary to store the genome lengths\n",
    "genome_lengths = {}\n",
    "\n",
    "# Calculate and store the length of each sequence\n",
    "for gpsc, fasta_file in gpscs.items():\n",
    "    records = list(SeqIO.parse(fasta_file, \"fasta\"))\n",
    "    genome_length = sum(len(record.seq) for record in records)\n",
    "    genome_lengths[gpsc] = genome_length  # Store the genome length in the dictionary\n",
    "\n",
    "#Print the genome lengths\n",
    "for gpsc, length in genome_lengths.items():\n",
    "    print(f\"{gpsc}: {length} bp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bb8b9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import yaml\n",
    "from Bio import SeqIO\n",
    "\n",
    "# Load the config file\n",
    "with open(\"config/TB_reps.yaml\", 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Extract the GPSCs\n",
    "gpscs = config['samples']\n",
    "\n",
    "# Open a CSV file to write\n",
    "with open('assembly_lengths.csv', mode='w', newline='') as csv_file:\n",
    "    # Create a CSV writer object\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    \n",
    "    # Calculate and store the length of each sequence, then write to CSV\n",
    "    for gpsc, fasta_file in gpscs.items():\n",
    "        records = list(SeqIO.parse(fasta_file, \"fasta\"))\n",
    "        genome_length = sum(len(record.seq) for record in records)\n",
    "        # Write the ID, start (0), and the length to the CSV file\n",
    "        csv_writer.writerow([gpsc, 0, genome_length])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
